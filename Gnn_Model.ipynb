{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdusTozG5CZh"
   },
   "source": [
    "\n",
    "# **Graph-Based Neural Network, Deep Learning Approach for Predicting Milling Process Times**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqP50MiI5DiL"
   },
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69876,
     "status": "ok",
     "timestamp": 1759935671619,
     "user": {
      "displayName": "lorenzo ravalli",
      "userId": "10548480941014227620"
     },
     "user_tz": -120
    },
    "id": "hANqOE0C9HsT",
    "outputId": "338ff267-13ca-4ea2-c709-a5b0619ce8d1"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cu124.html  #cu124\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.6.0+cu124.html  #cu124\n",
    "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
    "!pip install -q pyg-lib -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
    "!pip install -q torch-geometric\n",
    "!pip install -q trimesh\n",
    "!pip install -q fast_simplification\n",
    "!pip install -q wandb\n",
    "!pip install reportlab\n",
    "!pip install optuna plotly\n",
    "!pip install pymeshlab\n",
    "!pip install optuna-dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29477,
     "status": "ok",
     "timestamp": 1759935701100,
     "user": {
      "displayName": "lorenzo ravalli",
      "userId": "10548480941014227620"
     },
     "user_tz": -120
    },
    "id": "3s5YYEzPCoMq",
    "outputId": "424cc4f3-718c-4528-9874-569e5ba5bbd6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool, FeaStConv\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Dataset, Batch\n",
    "from torch_geometric.transforms import FaceToEdge\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_trimesh\n",
    "import torch_geometric.transforms as T\n",
    "import trimesh\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import csv\n",
    "from matplotlib.colors import Normalize\n",
    "import wandb\n",
    "import json\n",
    "from torch_geometric.loader.dataloader import Collater\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pymeshlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCxOzup45Xcy"
   },
   "source": [
    "## Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1759935708231,
     "user": {
      "displayName": "lorenzo ravalli",
      "userId": "10548480941014227620"
     },
     "user_tz": -120
    },
    "id": "hdwRLdSTCoMr"
   },
   "outputs": [],
   "source": [
    "def compute_shape_features_milling(mesh_obj, theta_deg=45.0):\n",
    "    \"\"\"\n",
    "    Shape features designed for 3-axis milling operations, robust to non-watertight meshes.\n",
    "    - Evaluates steepness with respect to Â±X, Â±Y, Â±Z and aggregates best/worst axis.\n",
    "    - Adds bbox, aspect ratios, SA/V, sphericity (IQ), convexity, sharp edges, inertia radii, and relative COM.\n",
    "    - Falls back to convex hull volume when the mesh volume is unreliable.\n",
    "    \"\"\"\n",
    "    m = mesh_obj.copy()\n",
    "\n",
    "    # Minimal cleaning for robustness\n",
    "    try:\n",
    "        m.remove_degenerate_faces()\n",
    "        m.remove_duplicate_faces()\n",
    "        m.remove_unreferenced_vertices()\n",
    "        m.remove_infinite_values()\n",
    "        m.rezero()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --- Volume and area (fallback to convex hull if needed) ---\n",
    "    used_hull = False\n",
    "    try:\n",
    "        volume = float(m.volume)\n",
    "        if not np.isfinite(volume) or volume <= 0:\n",
    "            raise ValueError\n",
    "    except Exception:\n",
    "        try:\n",
    "            volume = float(m.convex_hull.volume)\n",
    "            used_hull = True\n",
    "        except Exception:\n",
    "            volume = 0.0\n",
    "            used_hull = True\n",
    "\n",
    "    area = float(m.area) if np.isfinite(m.area) else 0.0\n",
    "\n",
    "    # --- Ratios and shape indices ---\n",
    "    sa_to_vol = (area / volume) if volume > 0 else 0.0\n",
    "    # Isoperimetric Quotient (1 for a sphere, 0..1)\n",
    "    isoperimetric_q = (36.0 * np.pi * (volume ** 2) / (area ** 3)) if (area > 0 and volume > 0) else 0.0\n",
    "    # Your \"compactness\" was the inverse of IQ; we keep it explicitly\n",
    "    compactness_inv = (area ** 3) / (36.0 * np.pi * (volume ** 2)) if (area > 0 and volume > 0) else 0.0\n",
    "\n",
    "    # --- Bounding box and aspect ratios ---\n",
    "    try:\n",
    "        l, w, h = [float(x) for x in m.extents]  # box length, width, height\n",
    "    except Exception:\n",
    "        l, w, h = 0.0, 0.0, 0.0\n",
    "    ext = np.array([l, w, h], dtype=float)\n",
    "    max_e = float(np.max(ext)) if np.all(np.isfinite(ext)) else 0.0\n",
    "    min_e = float(np.min(ext)) if np.all(np.isfinite(ext)) else 0.0\n",
    "    med_e = float(np.median(ext)) if np.all(np.isfinite(ext)) else 0.0\n",
    "    aspect_max_min = (max_e / (min_e + 1e-9)) if min_e > 0 else 0.0\n",
    "    aspect_max_med = (max_e / (med_e + 1e-9)) if med_e > 0 else 0.0\n",
    "\n",
    "    # --- Inertia and radii of gyration (more interpretable) ---\n",
    "    try:\n",
    "        I = np.array(m.principal_inertia_components, dtype=float)\n",
    "        I.sort()  # ascending order\n",
    "    except Exception:\n",
    "        I = np.zeros(3, dtype=float)\n",
    "    mass = volume  # unit density â†’ mass ~ volume\n",
    "    radii_g = np.sqrt(np.clip(I / (mass + 1e-12), 0.0, np.inf)) if mass > 0 else np.zeros(3, dtype=float)\n",
    "\n",
    "    # --- Center of mass relative to bounding box (0..1) ---\n",
    "    try:\n",
    "        com = np.array(m.center_mass, dtype=float)\n",
    "        bbox_min, bbox_max = m.bounds\n",
    "        bbox_size = np.maximum(bbox_max - bbox_min, 1e-9)\n",
    "        com_rel = (com - bbox_min) / bbox_size\n",
    "    except Exception:\n",
    "        com_rel = np.zeros(3, dtype=float)\n",
    "\n",
    "    # --- Convexity: ratio between mesh volume and convex hull volume ---\n",
    "    try:\n",
    "        hull_vol = float(m.convex_hull.volume)\n",
    "        convexity = (volume / hull_vol) if hull_vol > 0 else 1.0\n",
    "    except Exception:\n",
    "        convexity = 1.0\n",
    "\n",
    "    # --- Face normals and areas (for steepness & sharp edges) ---\n",
    "    try:\n",
    "        normals = m.face_normals  # (F, 3)\n",
    "        areas = m.area_faces      # (F,)\n",
    "        total_area = float(np.sum(areas))\n",
    "    except Exception:\n",
    "        normals = np.zeros((0, 3), dtype=float)\n",
    "        areas = np.zeros((0,), dtype=float)\n",
    "        total_area = 0.0\n",
    "\n",
    "    def steep_fraction(direction, theta_rad):\n",
    "        if total_area <= 0 or normals.shape[0] == 0:\n",
    "            return 0.0\n",
    "        c = normals @ direction  # cosine of the angle with the direction\n",
    "        mask = c < np.cos(theta_rad)  # angle > theta\n",
    "        return float(np.sum(areas[mask])) / total_area\n",
    "\n",
    "    theta = np.deg2rad(theta_deg)\n",
    "    axes = np.eye(3, dtype=float)\n",
    "    dirs = np.vstack([axes, -axes])  # Â±X, Â±Y, Â±Z\n",
    "    steep_fracs = np.array([steep_fraction(d, theta) for d in dirs]) if total_area > 0 else np.zeros(6, dtype=float)\n",
    "    steep_best = float(np.min(steep_fracs)) if steep_fracs.size else 0.0\n",
    "    steep_worst = float(np.max(steep_fracs)) if steep_fracs.size else 0.0\n",
    "\n",
    "    # Keep the equivalent of \"overhang\" vs +Z for compatibility with the old model\n",
    "    steep_posZ = steep_fracs[2] if steep_fracs.size >= 3 else 0.0\n",
    "\n",
    "    # --- Sharp edges: fraction of dihedral angles below a threshold (e.g., 30Â°) ---\n",
    "    try:\n",
    "        dihed = m.face_adjacency_angles  # radians\n",
    "        sharp_fraction = float(np.mean(dihed < np.deg2rad(30.0))) if dihed is not None and len(dihed) > 0 else 0.0\n",
    "    except Exception:\n",
    "        sharp_fraction = 0.0\n",
    "\n",
    "    # Pack everything into a flat dictionary (no nested arrays)\n",
    "    return {\n",
    "        # base\n",
    "        'volume': float(volume),\n",
    "        'surface_area': float(area),\n",
    "        'sa_to_vol': float(sa_to_vol),\n",
    "        'isoperimetric_q': float(isoperimetric_q),\n",
    "        'compactness_inv': float(compactness_inv),\n",
    "\n",
    "        # bbox\n",
    "        'bbox_len': float(l),\n",
    "        'bbox_wid': float(w),\n",
    "        'bbox_hei': float(h),\n",
    "        'aspect_max_min': float(aspect_max_min),\n",
    "        'aspect_max_med': float(aspect_max_med),\n",
    "\n",
    "        # inertia and radii\n",
    "        'principal_inertia_0': float(I[0]),\n",
    "        'principal_inertia_1': float(I[1]),\n",
    "        'principal_inertia_2': float(I[2]),\n",
    "        'radius_gyr_0': float(radii_g[0]),\n",
    "        'radius_gyr_1': float(radii_g[1]),\n",
    "        'radius_gyr_2': float(radii_g[2]),\n",
    "\n",
    "        # relative COM (0..1)\n",
    "        'center_mass_x_rel': float(com_rel[0]),\n",
    "        'center_mass_y_rel': float(com_rel[1]),\n",
    "        'center_mass_z_rel': float(com_rel[2]),\n",
    "\n",
    "        # convexity\n",
    "        'convexity': float(convexity),\n",
    "\n",
    "        # milling-oriented steepness\n",
    "        'steep_frac_best_axis': float(steep_best),\n",
    "        'steep_frac_worst_axis': float(steep_worst),\n",
    "        'steep_frac_posZ': float(steep_posZ),  # compatibility with original \"overhang\"\n",
    "\n",
    "        # sharp edges\n",
    "        'sharp_edge_fraction': float(sharp_fraction),\n",
    "\n",
    "        # diagnostics\n",
    "        'used_convex_hull_volume': bool(used_hull),\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_only(mesh):\n",
    "    \"\"\"\n",
    "    Normalize a trimesh mesh:\n",
    "    - Center the model at the origin\n",
    "    - Scale vertices so that the maximum radius is 1\n",
    "    Args:\n",
    "        mesh (trimesh.Trimesh)\n",
    "    Returns:\n",
    "        (mesh_normalized, scale_factor)\n",
    "    \"\"\"\n",
    "    m = mesh.copy()\n",
    "\n",
    "    # Center vertices at the origin\n",
    "    centroid = m.vertices.mean(axis=0)\n",
    "    m.vertices -= centroid\n",
    "\n",
    "    # Scale to fit within the unit sphere\n",
    "    max_dist = np.max(np.linalg.norm(m.vertices, axis=1))\n",
    "    if max_dist > 0:\n",
    "        m.vertices /= max_dist\n",
    "\n",
    "    return m, max_dist\n",
    "\n",
    "\n",
    "def z_score_norm_train(labels_df, target_cols=None, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Z-score normalize all numeric columns in the DataFrame,\n",
    "    excluding target or specified columns.\n",
    "    \"\"\"\n",
    "    df = labels_df.copy()\n",
    "    stats = {}\n",
    "\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    if target_cols is None:\n",
    "        target_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]) and col not in exclude_cols + target_cols:\n",
    "            stats[col] = {}\n",
    "            mean_val = df[col].mean()\n",
    "            std_val = df[col].std()\n",
    "            std_val = std_val if std_val > 0 else 1.0\n",
    "            df[col] = (df[col] - mean_val) / std_val\n",
    "            stats[col]['mean'] = mean_val\n",
    "            stats[col]['std'] = std_val\n",
    "\n",
    "    return df, stats\n",
    "\n",
    "\n",
    "#train_df, stats = z_score_norm_train(csv_train, target_cols=[\"tempo_minuti\"])\n",
    "# test_df = z_score_norm_test(csv_test, stats)\n",
    "\n",
    "def z_score_norm_test(test_data, train_stats):\n",
    "    \"\"\"\n",
    "    Normalize test data using the training statistics.\n",
    "    \"\"\"\n",
    "    df = test_data.copy()\n",
    "    for col, st in train_stats.items():\n",
    "        if col in df.columns:\n",
    "            mean_val, std_val = st['mean'], st['std']\n",
    "            df[col] = (df[col] - mean_val) / std_val\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1759935708286,
     "user": {
      "displayName": "lorenzo ravalli",
      "userId": "10548480941014227620"
     },
     "user_tz": -120
    },
    "id": "xBaW8E_qCoMr"
   },
   "outputs": [],
   "source": [
    "class STLDataset(Dataset):\n",
    "    def __init__(self, root, printcfg_labels_path, raw_dir=None, raw_simplified_dir=None,\n",
    "                 processed_dir=None, transform=None, pre_transform=None, pre_filter=None,\n",
    "                 raw_extension='.stl', dataset_norm_stats=None,\n",
    "                 exclude_features=None, exclude_pieces=None):\n",
    "\n",
    "        self.exclude_features = exclude_features or []\n",
    "        self.exclude_pieces = set(str(p) for p in (exclude_pieces or []))  # list of pieces to exclude\n",
    "\n",
    "        # Load CSV\n",
    "        self.print_cfg_labels = pd.read_csv(printcfg_labels_path, sep=\";\")\n",
    "\n",
    "        # Encode categorical columns\n",
    "        if 'pezzo_id' in self.print_cfg_labels.columns:\n",
    "            self.print_cfg_labels['pezzo_id'] = self.print_cfg_labels['pezzo_id'].astype(str)\n",
    "\n",
    "        # ðŸ”¹ Remove excluded pieces\n",
    "        if len(self.exclude_pieces) > 0:\n",
    "            self.print_cfg_labels = self.print_cfg_labels[~self.print_cfg_labels['pezzo_id'].isin(self.exclude_pieces)]\n",
    "\n",
    "        # ðŸ”¹ Embedding for process ID\n",
    "        if 'id operazione' in self.print_cfg_labels.columns:\n",
    "            original_cats = self.print_cfg_labels['id operazione'].astype('category')\n",
    "            self.proc_id_to_name = dict(enumerate(original_cats.cat.categories))\n",
    "            self.print_cfg_labels['id operazione'] = original_cats.cat.codes\n",
    "            self.n_processi = len(self.proc_id_to_name)\n",
    "\n",
    "        # ðŸ”¹ Normalization of tabular features (excluding time and process)\n",
    "        exclude_cols = [\"time\", \"id operazione\"]\n",
    "        if dataset_norm_stats is None:\n",
    "            self.print_cfg_labels, self.dataset_norm_stats = z_score_norm_train(\n",
    "                self.print_cfg_labels,\n",
    "                target_cols=[\"time\"],\n",
    "                exclude_cols=exclude_cols\n",
    "            )\n",
    "            self.training_dataset = True\n",
    "        else:\n",
    "            self.print_cfg_labels = z_score_norm_test(\n",
    "                self.print_cfg_labels,\n",
    "                dataset_norm_stats,\n",
    "                exclude_cols=exclude_cols\n",
    "            )\n",
    "            self.dataset_norm_stats = dataset_norm_stats\n",
    "            self.training_dataset = False\n",
    "\n",
    "        # ðŸ”¹ List of all available mesh features\n",
    "        self.all_features = [\n",
    "            'volume', 'surface_area', 'sa_to_vol', 'isoperimetric_q', 'compactness_inv',\n",
    "            'bbox_len', 'bbox_wid', 'bbox_hei', 'aspect_max_min', 'aspect_max_med',\n",
    "            'principal_inertia_0', 'principal_inertia_1', 'principal_inertia_2',\n",
    "            'radius_gyr_0', 'radius_gyr_1', 'radius_gyr_2',\n",
    "            'center_mass_x_rel', 'center_mass_y_rel', 'center_mass_z_rel',\n",
    "            'convexity', 'steep_frac_best_axis', 'steep_frac_worst_axis', 'steep_frac_posZ',\n",
    "            'sharp_edge_fraction', 'used_convex_hull_volume', 'scaling_factor',\n",
    "            'mrv', 'removal_ratio', 'sa_to_vol_finale'\n",
    "        ]\n",
    "\n",
    "        self.active_features = [f for f in self.all_features if f not in self.exclude_features]\n",
    "\n",
    "        # Directories\n",
    "        self._custom_raw_dir = raw_dir\n",
    "        self._custom_raw_simplified_dir = raw_simplified_dir\n",
    "        self._custom_processed_dir = processed_dir\n",
    "        self.raw_extension = raw_extension\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self):\n",
    "        return self._custom_raw_dir or os.path.join(self.root, 'raw')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        filenames = self.print_cfg_labels['pezzo_id'].unique()\n",
    "        stl_filenames = [f\"{fn}.stl\" if not fn.endswith('.stl') else fn for fn in filenames]\n",
    "        return sorted(stl_filenames)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        processed_files_path = os.path.join(self.processed_dir, \"processed_files.txt\")\n",
    "        if os.path.exists(processed_files_path):\n",
    "            with open(processed_files_path, 'r') as f:\n",
    "                processed_files = [line.strip() for line in f.readlines()]\n",
    "            return [f'{os.path.splitext(filename)[0]}.pt' for filename in processed_files]\n",
    "        else:\n",
    "            return [f'{os.path.splitext(filename)[0]}.pt' for filename in self.raw_file_names]\n",
    "\n",
    "    @property\n",
    "    def raw_simplified_dir(self):\n",
    "        return self._custom_raw_simplified_dir or self.raw_dir\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return self._custom_processed_dir or os.path.join(self.root, 'processed')\n",
    "\n",
    "    def process(self):\n",
    "        print('Processing...')\n",
    "        processed_files = []\n",
    "        data_list = []\n",
    "        all_features_dict = {feat: [] for feat in self.active_features}\n",
    "\n",
    "        for stl_path in tqdm(self.raw_file_names, desc=\"Processing STL files\"):\n",
    "            try:\n",
    "                # Original STL (for normalization and volumes)\n",
    "                raw_path = os.path.join(self.raw_dir, stl_path)\n",
    "                trimesh_raw = trimesh.load_mesh(raw_path)\n",
    "                result = normalize_only(trimesh_raw)\n",
    "                if result is None or result[0] is None:\n",
    "                    print(f\"Skip {stl_path}: normalize_only returned None\")\n",
    "                    continue\n",
    "                trimesh_raw_norm, scaling_factor = result\n",
    "\n",
    "                mesh_feat = compute_shape_features_milling(trimesh_raw_norm)\n",
    "                mesh_feat['scaling_factor'] = float(scaling_factor or 0.0)\n",
    "\n",
    "                # ðŸ”¹ Raw volume from CSV\n",
    "                pezzo_id = os.path.splitext(os.path.basename(stl_path))[0]\n",
    "                row = self.print_cfg_labels[self.print_cfg_labels['pezzo_id'] == pezzo_id].iloc[0]\n",
    "                vol_grezzo = float(row['volume grezzo [mm3]'])\n",
    "\n",
    "                # ðŸ”¹ Compute new features\n",
    "                vol_finale = mesh_feat.get(\"volume\", 0.0)\n",
    "                sa_finale = mesh_feat.get(\"surface_area\", 0.0)\n",
    "                mrv = max(0.0, vol_grezzo - vol_finale)\n",
    "                removal_ratio = mrv / (vol_grezzo + 1e-9)\n",
    "                sa_to_vol_finale = sa_finale / (vol_finale + 1e-9)\n",
    "\n",
    "                mesh_feat[\"mrv\"] = mrv\n",
    "                mesh_feat[\"removal_ratio\"] = removal_ratio\n",
    "                mesh_feat[\"sa_to_vol_finale\"] = sa_to_vol_finale\n",
    "\n",
    "                # Save for global normalization\n",
    "                for feat in self.active_features:\n",
    "                    all_features_dict[feat].append(mesh_feat.get(feat, 0.0))\n",
    "\n",
    "                # ðŸ”¹ Load simplified mesh\n",
    "                simp_path = os.path.join(self.raw_simplified_dir, stl_path)\n",
    "                trimesh_simplified = trimesh.load_mesh(simp_path)\n",
    "                trimesh_simplified.remove_degenerate_faces()\n",
    "                trimesh_simplified.remove_duplicate_faces()\n",
    "                trimesh_simplified.remove_unreferenced_vertices()\n",
    "\n",
    "                data = from_trimesh(trimesh_simplified)\n",
    "                if data is None or data.x is None:\n",
    "                    verts = trimesh_simplified.vertices\n",
    "                    faces = trimesh_simplified.faces\n",
    "                    edges = np.vstack([faces[:, [0, 1]], faces[:, [1, 2]], faces[:, [2, 0]]])\n",
    "                    edges = np.unique(np.sort(edges, axis=1), axis=0)\n",
    "                    edge_index = torch.tensor(edges.T, dtype=torch.long)\n",
    "                    x = torch.tensor(verts, dtype=torch.float)\n",
    "                    from torch_geometric.data import Data\n",
    "                    data = Data(x=x, edge_index=edge_index, face=torch.tensor(faces.T, dtype=torch.long))\n",
    "\n",
    "                data.pos = data.x.clone()\n",
    "                data.face_normals = torch.tensor(trimesh_simplified.face_normals, dtype=torch.float)\n",
    "                data.mesh_feat = mesh_feat\n",
    "\n",
    "                filename = os.path.basename(stl_path)\n",
    "                data.filename = os.path.splitext(filename)[0]\n",
    "                data_list.append(data)\n",
    "                processed_files.append(filename)\n",
    "            except Exception as e:\n",
    "                with open(\"processing_errors.log\", 'a') as f:\n",
    "                    f.write(f\"Error processing {stl_path}: {str(e)}\\n\")\n",
    "                continue\n",
    "\n",
    "        # ðŸ”¹ Compute normalization statistics\n",
    "        feature_stats = {}\n",
    "        for feature in self.active_features:\n",
    "            values = np.array(all_features_dict[feature])\n",
    "            feature_stats[feature] = {'mean': values.mean(), 'std': values.std() if values.std() > 0 else 1.0}\n",
    "\n",
    "        if self.training_dataset:\n",
    "            self.dataset_norm_stats.update(feature_stats)\n",
    "\n",
    "        # ðŸ”¹ Normalize features\n",
    "        for idx, data in enumerate(data_list):\n",
    "            for feature in self.active_features:\n",
    "                data.mesh_feat[feature] = (\n",
    "                    data.mesh_feat.get(feature, 0.0) - self.dataset_norm_stats[feature]['mean']\n",
    "                ) / self.dataset_norm_stats[feature]['std']\n",
    "\n",
    "            mesh_feat_values = [data.mesh_feat[f] for f in self.active_features]\n",
    "            data.mesh_feat = torch.tensor(mesh_feat_values, dtype=torch.float32)\n",
    "            torch.save(data, os.path.join(self.processed_dir, f'{data.filename}.pt'))\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.print_cfg_labels)\n",
    "\n",
    "    def get(self, idx):\n",
    "        row = self.print_cfg_labels.iloc[idx]\n",
    "        filename = row['pezzo_id']\n",
    "\n",
    "        # ðŸ”¹ Raw volume\n",
    "        volume_grezzo = torch.tensor([float(row['volume grezzo [mm3]'])], dtype=torch.float32)\n",
    "\n",
    "        # ðŸ”¹ Process as integer index (embedding)\n",
    "        proc_id = torch.tensor(int(row['id operazione']), dtype=torch.long)\n",
    "\n",
    "        # ðŸ”¹ Target (log time + z-score per process)\n",
    "        t_log = np.log1p(float(row[\"time\"]))\n",
    "        labels = torch.tensor([t_log], dtype=torch.float32)\n",
    "\n",
    "        # ðŸ”¹ Load corresponding graph\n",
    "        data_path = os.path.join(self.processed_dir, f\"{os.path.splitext(filename)[0]}.pt\")\n",
    "        data = torch.load(data_path, weights_only=False)\n",
    "\n",
    "        return data, volume_grezzo, proc_id, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FA-eNh9u55QT"
   },
   "source": [
    "## Simplification of STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Cp1-1iD56lF"
   },
   "outputs": [],
   "source": [
    "base_path = \"/content/gdrive/MyDrive/GNN_Def\"\n",
    "\n",
    "root_dir = base_path\n",
    "raw_dir = os.path.join(base_path, \"raw\")\n",
    "simplified_dir = os.path.join(base_path, \"raw_simplified2\")\n",
    "\n",
    "target_faces = 1024               # Desired number of faces in the simplified mesh\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(simplified_dir, exist_ok=True)\n",
    "\n",
    "# Simplification loop\n",
    "for filename in os.listdir(raw_dir):\n",
    "    if not filename.lower().endswith(\".stl\"):\n",
    "        continue  # Skip non-STL files\n",
    "\n",
    "    input_path = os.path.join(raw_dir, filename)\n",
    "    output_path = os.path.join(simplified_dir, filename)\n",
    "\n",
    "    try:\n",
    "        # --- Load the mesh ---\n",
    "        ms = pymeshlab.MeshSet()\n",
    "        ms.load_new_mesh(input_path)\n",
    "\n",
    "        # --- Simplify using quadratic edge collapse ---\n",
    "        ms.meshing_decimation_quadric_edge_collapse(\n",
    "            targetfacenum=target_faces,\n",
    "            preservenormal=True,\n",
    "            preservetopology=True,\n",
    "            qualitythr=0.3\n",
    "        )\n",
    "\n",
    "        # --- Save the simplified mesh ---\n",
    "        ms.save_current_mesh(output_path)\n",
    "        print(f\"Simplified: {filename} â†’ {target_faces} faces (saved to {output_path})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error simplifying {filename}: {e}\")\n",
    "\n",
    "print(\"\\n Simplification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52qBOnRm59t4"
   },
   "source": [
    "## Path and Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20131,
     "status": "ok",
     "timestamp": 1759935731136,
     "user": {
      "displayName": "lorenzo ravalli",
      "userId": "10548480941014227620"
     },
     "user_tz": -120
    },
    "id": "3LijktAg9NQO",
    "outputId": "6c1225aa-0fbb-4c0e-d981-4d3f4d296095"
   },
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# Files and directory || change if you have different path || here google Colab + google Drive\n",
    "# ===============================================================================================\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "base_path = \"/GNN_Milling\"\n",
    "\n",
    "root_dir = base_path\n",
    "raw_dir = os.path.join(base_path, \"raw\")\n",
    "raw_simplified_dir = os.path.join(base_path, \"raw_simplified\")\n",
    "processed_dir = os.path.join(base_path, \"processed\")\n",
    "labels_path = os.path.join(base_path, \"dataset.csv\")\n",
    "save_dir = os.path.join(base_path, \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4118,
     "status": "ok",
     "timestamp": 1759935735253,
     "user": {
      "displayName": "lorenzo ravalli",
      "userId": "10548480941014227620"
     },
     "user_tz": -120
    },
    "id": "R-N63gH-CoMs"
   },
   "outputs": [],
   "source": [
    "orig_dataset = STLDataset(\n",
    "    root=base_path,\n",
    "    raw_dir=raw_dir,\n",
    "    raw_simplified_dir=raw_simplified_dir,\n",
    "    processed_dir=processed_dir,\n",
    "    printcfg_labels_path=labels_path,\n",
    "    exclude_features=['volume', 'bbox_len', 'bbox_wid', 'bbox_hei', 'aspect_max_min', 'aspect_max_med',\n",
    "            'principal_inertia_0', 'principal_inertia_1', 'principal_inertia_2',\n",
    "            'center_mass_x_rel', 'center_mass_y_rel', 'center_mass_z_rel',\n",
    "            'steep_frac_best_axis', 'steep_frac_worst_axis', 'steep_frac_posZ',\n",
    "            'sharp_edge_fraction', 'used_convex_hull_volume', 'scaling_factor'],\n",
    "    exclude_pieces= ['4851D5091']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD5c4dqb6Io2"
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1759935739195,
     "user": {
      "displayName": "lorenzo ravalli",
      "userId": "10548480941014227620"
     },
     "user_tz": -120
    },
    "id": "9GrGQOu0CoMs"
   },
   "outputs": [],
   "source": [
    "class FeaStNet_TimePred(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, heads=11,\n",
    "                 hidden_dim=128, dropout=0.2,\n",
    "                 n_processi=15, proc_emb_dim=8):\n",
    "        super(FeaStNet_TimePred, self).__init__()\n",
    "\n",
    "        # --- GNN ---\n",
    "        self.fc0 = nn.Linear(in_channels, 16)\n",
    "        self.conv1 = FeaStConv(16, 32, heads=heads)\n",
    "        self.conv2 = FeaStConv(32, 64, heads=heads)\n",
    "        self.conv3 = FeaStConv(64, 128, heads=heads)\n",
    "\n",
    "        # --- Process embedding ---\n",
    "        self.proc_emb = nn.Embedding(n_processi, proc_emb_dim)\n",
    "\n",
    "        # --- Tabular encoder (mesh + volume + process) ---\n",
    "        self.mesh_feat_encoder = nn.Sequential(\n",
    "            nn.Linear(N_MESH_FEAT + 1 + proc_emb_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        # --- Fusion + regression head ---\n",
    "        self.norm_concat = nn.LayerNorm(128 + hidden_dim)\n",
    "        self.fc1 = nn.Linear(128 + hidden_dim, 256)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(256, out_channels)\n",
    "\n",
    "    def forward(self, pos, edge_index, batch, mesh_feat, volume_grezzo, proc_id):\n",
    "        # --- GNN embedding ---\n",
    "        x = F.elu(self.fc0(pos))\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)  # (B, 128)\n",
    "\n",
    "        B = x.size(0)\n",
    "\n",
    "        # --- Fix tensor shapes ---\n",
    "        if mesh_feat.dim() == 1:\n",
    "            mesh_feat = mesh_feat.view(B, -1)\n",
    "        elif mesh_feat.size(0) != B:\n",
    "            mesh_feat = mesh_feat.view(B, -1)\n",
    "\n",
    "        if volume_grezzo.dim() == 1:\n",
    "            volume_grezzo = volume_grezzo.unsqueeze(1)\n",
    "\n",
    "        # --- Process embedding ---\n",
    "        proc_vec = self.proc_emb(proc_id)  # (B, proc_emb_dim)\n",
    "\n",
    "        # --- Concatenate tabular features ---\n",
    "        extra_inputs = torch.cat([mesh_feat, volume_grezzo, proc_vec], dim=-1)\n",
    "        feat_encoded = self.mesh_feat_encoder(extra_inputs)\n",
    "\n",
    "        # --- Fusion with GNN output ---\n",
    "        out = torch.cat([x, feat_encoded], dim=-1)\n",
    "        out = self.norm_concat(out)\n",
    "        out = F.elu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPlNfkDxqnlg"
   },
   "source": [
    "## LOPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1759935740536,
     "user": {
      "displayName": "lorenzo ravalli",
      "userId": "10548480941014227620"
     },
     "user_tz": -120
    },
    "id": "zBQrudvPV0I2"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# GLOBAL PARAMETERS\n",
    "# ==============================\n",
    "N_MESH_FEAT = len(orig_dataset.active_features)\n",
    "RESULTS_DIR = save_dir\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "\n",
    "# Fixed generator for DataLoader\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "# ==============================\n",
    "# METRICS\n",
    "# ==============================\n",
    "def compute_metrics(preds, labels):\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    mse = np.mean((preds - labels) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    mape = np.mean(np.abs((labels - preds) / (labels + 1e-8))) * 100\n",
    "    wape = (np.sum(np.abs(labels - preds)) / (np.sum(np.abs(labels)) + 1e-8)) * 100\n",
    "    r2 = r2_score(labels, preds)\n",
    "    return mse, rmse, mae, mape, wape, r2\n",
    "\n",
    "# ==============================\n",
    "# LABEL NORMALIZATION BY PROCESS\n",
    "# ==============================\n",
    "def build_proc_stats(dataset):\n",
    "    \"\"\"Compute mean/std of log labels per process ONLY on training set\"\"\"\n",
    "    proc_stats = {}\n",
    "    for _, _, proc_id, labels in dataset:\n",
    "        val = labels.item()  # log\n",
    "        pid = int(proc_id.item())\n",
    "        if pid not in proc_stats:\n",
    "            proc_stats[pid] = []\n",
    "        proc_stats[pid].append(val)\n",
    "    for pid in proc_stats:\n",
    "        arr = np.array(proc_stats[pid])\n",
    "        proc_stats[pid] = {\n",
    "            \"mean\": arr.mean(),\n",
    "            \"std\": arr.std() if arr.std() > 1e-8 else 1.0\n",
    "        }\n",
    "    return proc_stats\n",
    "\n",
    "def normalize_labels(labels, proc_id, proc_stats):\n",
    "    mean = torch.tensor([proc_stats[int(pid.item())][\"mean\"] for pid in proc_id],\n",
    "                        device=labels.device, dtype=torch.float32).view(-1,1)\n",
    "    std  = torch.tensor([proc_stats[int(pid.item())][\"std\"] for pid in proc_id],\n",
    "                        device=labels.device, dtype=torch.float32).view(-1,1)\n",
    "    return (labels - mean) / std\n",
    "\n",
    "def denormalize_preds(preds, proc_id, proc_stats):\n",
    "    mean = torch.tensor([proc_stats[int(pid)][\"mean\"] for pid in proc_id],\n",
    "                        device=preds.device, dtype=torch.float32).view(-1,1)\n",
    "    std  = torch.tensor([proc_stats[int(pid)][\"std\"] for pid in proc_id],\n",
    "                        device=preds.device, dtype=torch.float32).view(-1,1)\n",
    "    return preds * std + mean\n",
    "\n",
    "# ==============================\n",
    "# AUXILIARY FUNCTIONS\n",
    "# ==============================\n",
    "def _unpack_batch(batch):\n",
    "    data, volume_grezzo, proc_id, labels = batch\n",
    "    if getattr(data, 'pos', None) is None and getattr(data, 'x', None) is not None:\n",
    "        data.pos = data.x\n",
    "    return data, volume_grezzo, proc_id, labels\n",
    "\n",
    "# ==============================\n",
    "# TRAIN + TEST\n",
    "# ==============================\n",
    "def train(model, loader, optimizer, criterion, proc_stats):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        data, volume_grezzo, proc_id, labels = _unpack_batch(batch)\n",
    "        data, volume_grezzo, proc_id, labels = (\n",
    "            data.to(device), volume_grezzo.to(device),\n",
    "            proc_id.to(device), labels.to(device).view(-1, 1)  # log\n",
    "        )\n",
    "        labels_norm = normalize_labels(labels, proc_id, proc_stats)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.pos, data.edge_index, data.batch,\n",
    "                    data.mesh_feat, volume_grezzo, proc_id)\n",
    "        loss = criterion(out, labels_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        loss.backward(); optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def test(model, loader, criterion, proc_stats):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_procs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            data, volume_grezzo, proc_id, labels = _unpack_batch(batch)\n",
    "            data, volume_grezzo, proc_id, labels = (\n",
    "                data.to(device), volume_grezzo.to(device),\n",
    "                proc_id.to(device), labels.to(device).view(-1, 1)  # log\n",
    "            )\n",
    "            preds_norm = model(data.pos, data.edge_index, data.batch,\n",
    "                               data.mesh_feat, volume_grezzo, proc_id)\n",
    "\n",
    "            preds_log = denormalize_preds(preds_norm, proc_id.cpu(), proc_stats)\n",
    "\n",
    "            all_preds.append(preds_log.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_procs.append(proc_id.cpu())\n",
    "\n",
    "    preds_log = torch.cat(all_preds, dim=0).numpy()\n",
    "    labels_log = torch.cat(all_labels, dim=0).numpy()\n",
    "    procs = torch.cat(all_procs, dim=0).numpy()\n",
    "\n",
    "    # ðŸ”¹ from log â†’ real scale\n",
    "    preds = np.expm1(preds_log)\n",
    "    labels = np.expm1(labels_log)\n",
    "\n",
    "    mse = np.mean((preds - labels) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    mape = np.mean(np.abs((labels - preds) / (labels + 1e-8))) * 100\n",
    "    wape = (np.sum(np.abs(labels - preds)) / (np.sum(np.abs(labels)) + 1e-8)) * 100\n",
    "    r2 = r2_score(labels, preds)\n",
    "    return mse, rmse, mae, mape, wape, r2, preds, labels, procs\n",
    "\n",
    "def custom_collate(batch):\n",
    "    datas, vols, procs, labels = zip(*batch)\n",
    "    batch_data = Collater(dataset=None, follow_batch=[])(list(datas))\n",
    "    vols   = torch.cat(vols, dim=0).float()\n",
    "    procs  = torch.cat(procs, dim=0).long()\n",
    "    labels = torch.cat(labels, dim=0).float().view(-1, 1)  # log\n",
    "    return batch_data, vols, procs, labels\n",
    "\n",
    "# ==============================\n",
    "# TRAIN + TEST ON ONE PIECE (SWA) + PLOT\n",
    "# ==============================\n",
    "\n",
    "swa_start_perc = 0.8\n",
    "swa_end_perc = 1.0\n",
    "epochs = 200\n",
    "\n",
    "swa_start = int(swa_start_perc * epochs)\n",
    "swa_end = int(swa_end_perc * epochs)\n",
    "\n",
    "def run_one_piece(pezzo_escluso, orig_dataset, results_all, n_runs=3, swa_start=swa_start, swa_end=swa_end):\n",
    "    dataset_holdout, dataset_train = [], []\n",
    "    for i in range(len(orig_dataset)):\n",
    "        data, volume_grezzo, proc_id, labels = orig_dataset[i]\n",
    "        if data.filename == pezzo_escluso:\n",
    "            dataset_holdout.append((data, volume_grezzo, proc_id, labels))\n",
    "        else:\n",
    "            dataset_train.append((data, volume_grezzo, proc_id, labels))\n",
    "\n",
    "    # Compute mean/std per process ONLY on train set\n",
    "    proc_stats = build_proc_stats(dataset_train)\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True,\n",
    "                              collate_fn=custom_collate, drop_last=False, generator=g)\n",
    "    holdout_loader = DataLoader(dataset_holdout, batch_size=1, shuffle=False,\n",
    "                                collate_fn=custom_collate, generator=g)\n",
    "\n",
    "    config = {\"hidden_dim\": 128, \"dropout\": 0.5, \"lr\": 0.0005}\n",
    "    all_runs_metrics = []\n",
    "    rows_csv = []\n",
    "    all_histories = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        print(f\"â–¶ Run {run+1}, piece {pezzo_escluso}, cfg={config}\")\n",
    "\n",
    "        model = FeaStNet_TimePred(\n",
    "            in_channels=3, out_channels=1,\n",
    "            hidden_dim=config[\"hidden_dim\"], dropout=config[\"dropout\"], heads=8,\n",
    "            n_processi=orig_dataset.n_processi,\n",
    "            proc_emb_dim=8\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=1e-4)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        history_mae = []\n",
    "        swa_state, swa_n = None, 0\n",
    "        for epoch in range(1,201):\n",
    "            train(model, train_loader, optimizer, criterion, proc_stats)\n",
    "            # compute MAE on holdout at each epoch\n",
    "            _, _, mae, _, _, _, _, _, _ = test(model, holdout_loader, criterion, proc_stats)\n",
    "            history_mae.append(mae)\n",
    "\n",
    "            if swa_start <= epoch <= swa_end:\n",
    "                state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                if swa_state is None:\n",
    "                    swa_state = state_dict\n",
    "                else:\n",
    "                    for k in swa_state:\n",
    "                        swa_state[k] += state_dict[k]\n",
    "                swa_n += 1\n",
    "\n",
    "        if swa_state is not None and swa_n > 0:\n",
    "            for k in swa_state:\n",
    "                swa_state[k] /= swa_n\n",
    "            model.load_state_dict(swa_state, strict=True)\n",
    "\n",
    "        mse, rmse, mae, mape, wape, r2, preds, labels, procs = test(\n",
    "            model, holdout_loader, criterion, proc_stats\n",
    "        )\n",
    "        all_histories.append(history_mae)\n",
    "\n",
    "        run_result = {\"run\": run+1, \"piece\": pezzo_escluso, \"mse\": mse, \"rmse\": rmse,\n",
    "                      \"mae\": mae, \"mape\": mape, \"wape\": wape, \"r2\": r2}\n",
    "        all_runs_metrics.append(run_result)\n",
    "\n",
    "        # ðŸ”¹ single predictions\n",
    "        for i in range(len(preds)):\n",
    "            rows_csv.append({\n",
    "                \"piece\": pezzo_escluso, \"run\": run+1,\n",
    "                \"process\": orig_dataset.proc_id_to_name[int(procs[i])],\n",
    "                \"real\": f\"{labels[i][0]:.2f}\",\n",
    "                \"predicted\": f\"{preds[i][0]:.2f}\",\n",
    "                \"error\": f\"{(preds[i][0]-labels[i][0]):+.2f}\",\n",
    "                \"error_%\": f\"{((preds[i][0]-labels[i][0])/labels[i][0]*100 if labels[i][0]!=0 else 0):+.2f}%\",\n",
    "            })\n",
    "\n",
    "        # ðŸ”¹ summary row\n",
    "        rows_csv.append({\n",
    "            \"piece\": pezzo_escluso,\n",
    "            \"run\": run+1,\n",
    "            \"process\": \"----\",\n",
    "            \"real\": \"----\",\n",
    "            \"predicted\": \"----\",\n",
    "            \"error\": f\"MAE={mae:.2f}\",\n",
    "            \"error_%\": f\"MAPE={mape:.2f}%\",\n",
    "            \"WAPE\": f\"{wape:.2f}%\"\n",
    "        })\n",
    "\n",
    "    # ðŸ”¹ save detailed CSV\n",
    "    csv_path = os.path.join(RESULTS_DIR, f\"predictions_{pezzo_escluso}_{timestamp}.csv\")\n",
    "    pd.DataFrame(rows_csv).to_csv(csv_path, index=False)\n",
    "    print(f\"ðŸ“‘ Predictions CSV saved: {csv_path}\")\n",
    "    results_all.extend(all_runs_metrics)\n",
    "\n",
    "    # ðŸ”¹ save MAE trend plot\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for run_idx, history in enumerate(all_histories):\n",
    "        plt.plot(history, label=f\"Run {run_idx+1}\")\n",
    "    plt.title(f\"MAE Trend - Piece {pezzo_escluso}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MAE (holdout)\")\n",
    "    plt.legend()\n",
    "    plot_path = os.path.join(RESULTS_DIR, f\"mae_plot_{pezzo_escluso}_{timestamp}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ“Š MAE plot saved: {plot_path}\")\n",
    "\n",
    "# ==============================\n",
    "# LOOP OVER ALL PIECES\n",
    "# ==============================\n",
    "def run_all_pieces(orig_dataset):\n",
    "    pezzi_unici = list({orig_dataset[i][0].filename for i in range(len(orig_dataset))})\n",
    "    results_all = []\n",
    "    for pezzo in pezzi_unici:\n",
    "        print(f\"\\n Leave-One-Piece-Out: excluded {pezzo}\")\n",
    "        run_one_piece(pezzo, orig_dataset, results_all, n_runs=5)\n",
    "\n",
    "    global_csv = os.path.join(RESULTS_DIR, f\"global_metrics_{timestamp}.csv\")\n",
    "    pd.DataFrame(results_all)[[\"piece\", \"run\", \"mae\", \"mape\", \"wape\", \"r2\"]].to_csv(global_csv, index=False)\n",
    "    print(f\"Global CSV saved at {global_csv}\")\n",
    "\n",
    "\n",
    "def run_selected_pieces(orig_dataset, selected_pieces):\n",
    "    results_all = []\n",
    "    for piece in selected_pieces:\n",
    "        print(f\"\\n Testing selected piece: {piece}\")\n",
    "        run_one_piece(piece, orig_dataset, results_all, n_runs=2)  # can increase to 5 if desired\n",
    "    # save only selected pieces results\n",
    "    timestamp_sel = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = os.path.join(RESULTS_DIR, f\"selected_metrics_{timestamp_sel}.csv\")\n",
    "    pd.DataFrame(results_all)[[\"piece\", \"run\", \"mae\", \"mape\", \"wape\", \"r2\"]].to_csv(csv_path, index=False)\n",
    "    print(f\" Selected metrics saved at {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1lVTF0HOzNg"
   },
   "outputs": [],
   "source": [
    "run_all_pieces(orig_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTNS4bYb7AD8"
   },
   "source": [
    "## Classical Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 767,
     "status": "ok",
     "timestamp": 1759928954821,
     "user": {
      "displayName": "lorenzo ra",
      "userId": "08902704832947926442"
     },
     "user_tz": -120
    },
    "id": "U59TmwTf7E-i",
    "outputId": "15c9589d-838b-4781-8d59-3b7350093344"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CSV Export for Classical Models\n",
    "# ==============================\n",
    "\n",
    "def _to_float(x):\n",
    "    \"\"\"Safely convert a tensor or scalar to a float.\"\"\"\n",
    "    try:\n",
    "        return float(x.item())\n",
    "    except AttributeError:\n",
    "        return float(x)\n",
    "\n",
    "@torch.no_grad()\n",
    "def export_mesh_features(\n",
    "    dataset,\n",
    "    output_path,\n",
    "    labels_in_log1p=True,\n",
    "    sort_output=True,\n",
    "    print_every=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Export mesh-level averaged features with process names and target times.\n",
    "    Each row corresponds to one dataset sample.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    feat_dim = None\n",
    "    n = len(dataset)\n",
    "\n",
    "    for idx in range(n):\n",
    "        data, volume, proc_id, label = dataset[idx]\n",
    "\n",
    "        # --- Mesh features ---\n",
    "        mesh = getattr(data, \"mesh_feat\", None)\n",
    "        if mesh is None:\n",
    "            raise ValueError(\"data.mesh_feat not found in dataset element.\")\n",
    "\n",
    "        # Ensure tensor shape\n",
    "        if mesh.dim() == 2:\n",
    "            mesh_mean = mesh.mean(dim=0).cpu().numpy().ravel()\n",
    "        elif mesh.dim() == 1:\n",
    "            mesh_mean = mesh.cpu().numpy().ravel()\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected mesh_feat shape: {tuple(mesh.shape)}\")\n",
    "\n",
    "        # Ensure consistent feature dimension\n",
    "        if feat_dim is None:\n",
    "            feat_dim = mesh_mean.shape[0]\n",
    "        elif mesh_mean.shape[0] != feat_dim:\n",
    "            raise ValueError(\n",
    "                f\"Inconsistent mesh_feat dimension: {feat_dim} vs {mesh_mean.shape[0]} (idx={idx})\"\n",
    "            )\n",
    "\n",
    "        # --- Tabular data ---\n",
    "        vol = _to_float(volume)\n",
    "        pid = int(_to_float(proc_id))\n",
    "        process_name = dataset.proc_id_to_name.get(pid, f\"proc_{pid}\")\n",
    "\n",
    "        if labels_in_log1p:\n",
    "            time_real = float(np.expm1(_to_float(label)))  # convert from log1p scale\n",
    "        else:\n",
    "            time_real = _to_float(label)\n",
    "\n",
    "        piece_name = str(getattr(data, \"filename\", f\"piece_{idx}\"))\n",
    "\n",
    "        # --- Build row ---\n",
    "        feat_dict = {f\"meshfeat_{j}\": mesh_mean[j] for j in range(feat_dim)}\n",
    "        feat_dict.update({\n",
    "            \"piece\": piece_name,\n",
    "            \"process\": process_name,\n",
    "            \"volume\": vol,\n",
    "            \"time_real\": time_real,\n",
    "        })\n",
    "        rows.append(feat_dict)\n",
    "\n",
    "        # --- Progress print ---\n",
    "        if print_every and ((idx + 1) % print_every == 0 or (idx + 1) == n):\n",
    "            print(f\"Processed {idx+1}/{n} samples\", flush=True)\n",
    "\n",
    "    # --- Build DataFrame ---\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # --- Optional sorting ---\n",
    "    if sort_output:\n",
    "        df = df.sort_values([\"piece\", \"process\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    # --- Save CSV ---\n",
    "    os.makedirs(os.path.dirname(output_path) or \".\", exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    # --- Summary ---\n",
    "    n_pieces = df[\"piece\"].nunique()\n",
    "    n_procs = df[\"process\"].nunique()\n",
    "    print(f\"\\nâœ… Exported {len(df)} rows | {n_pieces} pieces | {n_procs} processes | {feat_dim} mesh features\")\n",
    "    print(f\"ðŸ“‚ Saved to: {output_path}\")\n",
    "\n",
    "\n",
    "train_dataset = orig_dataset\n",
    "output_path = os.path.join(base_path, \"Classical_Results/csvClassical.csv\")\n",
    "export_mesh_features(train_dataset, output_path, labels_in_log1p=True, sort_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12018,
     "status": "ok",
     "timestamp": 1759928969170,
     "user": {
      "displayName": "lorenzo ra",
      "userId": "08902704832947926442"
     },
     "user_tz": -120
    },
    "id": "-x0rtZwx7Hft",
    "outputId": "4c8f7153-3ce3-4ccd-98ac-a16680979940"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "INPUT_CSV = os.path.join(base_path, \"Classical_Results/csvClassical.csv\")      # <-- your exported file\n",
    "RESULTS_DIR = os.path.join(base_path, \"Classical_Results\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ==============================\n",
    "# LOAD DATA\n",
    "# ==============================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "mesh_cols = [c for c in df.columns if c.startswith(\"meshfeat_\")]\n",
    "target_col = \"time_real\"\n",
    "piece_col = \"piece\"\n",
    "proc_col = \"process\"    # âœ… now using process *name*\n",
    "num_cols = mesh_cols + [\"volume\"]\n",
    "\n",
    "print(f\"ðŸ“‚ Loaded dataset: {df.shape[0]} rows, {len(mesh_cols)} mesh features, {df[piece_col].nunique()} unique pieces\")\n",
    "\n",
    "# ==============================\n",
    "# METRICS\n",
    "# ==============================\n",
    "def _safe_mape(y, yhat):\n",
    "    return np.mean(np.abs((yhat - y) / (y + 1e-8))) * 100\n",
    "\n",
    "def _wape(y, yhat):\n",
    "    return np.sum(np.abs(yhat - y)) / (np.sum(np.abs(y)) + 1e-8) * 100\n",
    "\n",
    "# ==============================\n",
    "# LOOP: Leave-One-Piece-Out\n",
    "# ==============================\n",
    "results, all_preds = [], []\n",
    "unique_pieces = df[piece_col].unique()\n",
    "print(f\"\\nðŸ”§ Found {len(unique_pieces)} unique pieces â†’ LOPO setup\\n\")\n",
    "\n",
    "for held_out in unique_pieces:\n",
    "    print(f\"ðŸš€ Leave-One-Piece-Out: excluding '{held_out}'\")\n",
    "\n",
    "    # Split train/test\n",
    "    train_df = df[df[piece_col] != held_out]\n",
    "    test_df  = df[df[piece_col] == held_out]\n",
    "\n",
    "    # --- Feature scaling ---\n",
    "    scaler = StandardScaler().fit(train_df[num_cols])\n",
    "    X_train = scaler.transform(train_df[num_cols])\n",
    "    X_test  = scaler.transform(test_df[num_cols])\n",
    "\n",
    "    # --- One-hot encode process names ---\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False).fit(train_df[[proc_col]])\n",
    "    X_train_proc = ohe.transform(train_df[[proc_col]])\n",
    "    X_test_proc  = ohe.transform(test_df[[proc_col]])\n",
    "\n",
    "    # --- Combine numeric + categorical ---\n",
    "    X_train = np.concatenate([X_train, X_train_proc], axis=1)\n",
    "    X_test  = np.concatenate([X_test, X_test_proc], axis=1)\n",
    "\n",
    "    y_train = train_df[target_col].values\n",
    "    y_test  = test_df[target_col].values\n",
    "\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1),\n",
    "        \"SVM\": SVR(C=10, gamma=\"scale\"),\n",
    "        \"DecisionTree\": DecisionTreeRegressor(max_depth=None, random_state=42)\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        mape = _safe_mape(y_test, preds)\n",
    "        wape = _wape(y_test, preds)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "\n",
    "        results.append({\n",
    "            \"piece\": held_out,\n",
    "            \"model\": name,\n",
    "            \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae,\n",
    "            \"MAPE%\": mape, \"WAPE%\": wape, \"R2\": r2\n",
    "        })\n",
    "\n",
    "        df_pred = pd.DataFrame({\n",
    "            \"piece\": held_out,\n",
    "            \"model\": name,\n",
    "            \"process\": test_df[proc_col].values,   # âœ… process name\n",
    "            \"real\": y_test,\n",
    "            \"pred\": preds\n",
    "        })\n",
    "        df_pred[\"error\"] = df_pred[\"pred\"] - df_pred[\"real\"]\n",
    "        df_pred[\"error_%\"] = np.where(df_pred[\"real\"] != 0,\n",
    "                                      df_pred[\"error\"] / df_pred[\"real\"] * 100,\n",
    "                                      np.nan)\n",
    "        all_preds.append(df_pred)\n",
    "\n",
    "        print(f\"   â–¶ {name:12s} | RMSE={rmse:8.2f} | MAE={mae:8.2f} | RÂ²={r2:6.3f}\")\n",
    "\n",
    "# ==============================\n",
    "# SAVE RESULTS\n",
    "# ==============================\n",
    "df_metrics = pd.DataFrame(results)\n",
    "df_preds = pd.concat(all_preds, ignore_index=True)\n",
    "\n",
    "metrics_csv = os.path.join(RESULTS_DIR, f\"classical_metrics_{timestamp}.csv\")\n",
    "preds_csv = os.path.join(RESULTS_DIR, f\"classical_predictions_{timestamp}.csv\")\n",
    "df_metrics.to_csv(metrics_csv, index=False)\n",
    "df_preds.to_csv(preds_csv, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Metrics saved to: {metrics_csv}\")\n",
    "print(f\"ðŸ“‘ Detailed predictions saved to: {preds_csv}\")\n",
    "\n",
    "# ==============================\n",
    "# SUMMARY\n",
    "# ==============================\n",
    "summary = (\n",
    "    df_metrics.groupby(\"model\")[[\"RMSE\", \"MAE\", \"MAPE%\", \"WAPE%\", \"R2\"]]\n",
    "    .mean()\n",
    "    .sort_values(\"R2\", ascending=False)\n",
    ")\n",
    "print(\"\\n=== ðŸ“Š Summary per model (average over all pieces) ===\")\n",
    "print(summary.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvc3e9qE80zX"
   },
   "source": [
    "## Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90986,
     "status": "ok",
     "timestamp": 1759929380299,
     "user": {
      "displayName": "lorenzo ra",
      "userId": "08902704832947926442"
     },
     "user_tz": -120
    },
    "id": "fzUbo3Ae83z1",
    "outputId": "0bd9fb08-dc84-474e-e78e-2ddb06f04f71"
   },
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# CORRELATION ANALYSIS: Global Mesh Features â†” Time\n",
    "# (Clustered correlation map without 'time' in heatmap)\n",
    "# =====================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "base_path = \"/Users/lorenzo/Desktop/GNN_Milling\"\n",
    "\n",
    "raw_dir = os.path.join(base_path, \"raw\")\n",
    "raw_simplified_dir = os.path.join(base_path, \"raw_simplified\")\n",
    "processed_dir = os.path.join(base_path, \"processed_FeatureSelection\")\n",
    "labels_path = os.path.join(base_path, \"dataset.csv\")\n",
    "pdf_path = os.path.join(base_path, \"GNNcorrelations_global.pdf\")\n",
    "\n",
    "# --- Load dataset ---\n",
    "orig_dataset2 = STLDataset(\n",
    "    root=base_path,\n",
    "    raw_dir=raw_dir,\n",
    "    raw_simplified_dir=raw_simplified_dir,\n",
    "    processed_dir=processed_dir,\n",
    "    printcfg_labels_path=labels_path,\n",
    "    exclude_features=[],\n",
    "    exclude_pieces=['4851D5091']\n",
    ")\n",
    "\n",
    "# --- Mesh features ---\n",
    "mesh_features = [\n",
    "    'volume', 'surface_area', 'sa_to_vol', 'isoperimetric_q', 'compactness_inv',\n",
    "    'bbox_len', 'bbox_wid', 'bbox_hei', 'aspect_max_min', 'aspect_max_med',\n",
    "    'principal_inertia_0', 'principal_inertia_1', 'principal_inertia_2',\n",
    "    'radius_gyr_0', 'radius_gyr_1', 'radius_gyr_2',\n",
    "    'center_mass_x_rel', 'center_mass_y_rel', 'center_mass_z_rel',\n",
    "    'convexity', 'steep_frac_best_axis', 'steep_frac_worst_axis',\n",
    "    'steep_frac_posZ', 'sharp_edge_fraction'\n",
    "]\n",
    "\n",
    "# --- Build global DataFrame ---\n",
    "records = []\n",
    "for i in range(len(orig_dataset2)):\n",
    "    data, volume_grezzo, proc_id, labels = orig_dataset2.get(i)\n",
    "    row = {\"process_id\": float(proc_id.item()), \"time\": float(labels.item())}\n",
    "    for j, feat in enumerate(mesh_features):\n",
    "        row[feat] = float(data.mesh_feat[j].item())\n",
    "    records.append(row)\n",
    "\n",
    "df_full = pd.DataFrame(records)\n",
    "\n",
    "# --- Compute correlation matrix ---\n",
    "corr_full = df_full[mesh_features + [\"time\"]].corr()\n",
    "\n",
    "# --- Helper: classify correlation strength ---\n",
    "def classify_corr(value):\n",
    "    abs_val = abs(value)\n",
    "    if abs_val < 0.3:\n",
    "        return \"Weak\"\n",
    "    elif abs_val < 0.6:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"Strong\"\n",
    "\n",
    "# --- Extract correlations with time only ---\n",
    "corr_time = corr_full[\"time\"].drop(\"time\").sort_values(key=lambda x: abs(x), ascending=False)\n",
    "corr_table = pd.DataFrame({\n",
    "    \"Feature\": corr_time.index,\n",
    "    \"Correlation with time\": corr_time.values,\n",
    "    \"Strength\": [classify_corr(v) for v in corr_time.values]\n",
    "})\n",
    "\n",
    "# --- Correlation matrix without 'time' for visualization ---\n",
    "corr_no_time = corr_full.drop(index=\"time\", columns=\"time\")\n",
    "\n",
    "# --- Save all results to PDF ---\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    # 1ï¸âƒ£ Clustered heatmap of feature correlations (without 'time')\n",
    "    sns.set(font_scale=0.8)\n",
    "    cluster = sns.clustermap(\n",
    "        corr_no_time,\n",
    "        cmap=\"coolwarm_r\",\n",
    "        center=0,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        figsize=(12, 10),\n",
    "        linewidths=0.3,\n",
    "        cbar_pos=(0.02, 0.8, 0.03, 0.18),\n",
    "        dendrogram_ratio=(.2, .1)\n",
    "    )\n",
    "    pdf.savefig(cluster.fig)\n",
    "    plt.close()\n",
    "\n",
    "    # 2ï¸âƒ£ Bar plot of correlation with time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        x=corr_time.values,\n",
    "        y=corr_time.index,\n",
    "        hue=[classify_corr(v) for v in corr_time.values],\n",
    "        dodge=False,\n",
    "        palette=\"Blues_r\"\n",
    "    )\n",
    "    plt.axvline(0, color=\"black\", linewidth=1)\n",
    "    plt.title(\"Feature â†” Time Correlation (All Processes Combined)\")\n",
    "    plt.xlabel(\"Correlation coefficient\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.legend(title=\"Strength\")\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(); plt.close()\n",
    "\n",
    "    # 3ï¸âƒ£ Correlation table (values and classification)\n",
    "    fig, ax = plt.subplots(figsize=(8, len(corr_table) * 0.3))\n",
    "    ax.axis('off')\n",
    "    table = ax.table(\n",
    "        cellText=corr_table.values,\n",
    "        colLabels=corr_table.columns,\n",
    "        loc='center'\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(8)\n",
    "    table.scale(1, 1.2)\n",
    "    plt.title(\"Correlation Table (All Processes Combined)\")\n",
    "    pdf.savefig(); plt.close()\n",
    "\n",
    "print(f\"âœ… Clustered correlation PDF saved at: {pdf_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RqP50MiI5DiL",
    "eCxOzup45Xcy",
    "FA-eNh9u55QT",
    "52qBOnRm59t4",
    "oD5c4dqb6Io2",
    "jTNS4bYb7AD8",
    "Mvc3e9qE80zX"
   ],
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7028576,
     "sourceId": 11682662,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
